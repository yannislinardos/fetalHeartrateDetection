{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Performance_metrics.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMMLV/egpERzh/vgevIXz+0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_hE3PpSjNfN3"},"source":["This module contains several useful functions that measure the performance of a model."]},{"cell_type":"code","metadata":{"id":"D2nPvlpmd_7M"},"source":["import numpy as np\n","import h5py\n","import os\n","from pickle import dump, load\n","from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_curve, auc\n","import seaborn as sns\n","from tqdm.notebook import trange, tqdm\n","\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RcfXZO7fTuk"},"source":["if __name__ == '__main__' and '__file__' not in globals():\n","  %run Code/Final/DataPreparation.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAq8UDIygJOV"},"source":["'''\n","INPUT\n","data: scaled data with input shape that depends on the model\n","model: an autoencoder which will be used to reconstruct the input data\n","error_fun: Mean Square Error or Mean Absolute Error\n","OUTPUT\n","errors: The reconstruction errors of the input data using the given autoencoder\n","'''\n","def find_errors(data, model, error_fun='mse'):\n","\n","  decoded = model.predict(data)\n","\n","  # errors = None\n","  # if error_fun == 'mse':\n","  #   errors = tf.keras.losses.MSE(data, decoded)\n","  # elif error_fun == 'mae':\n","  #   errors = tf.keras.losses.MAE(data, decoded)\n","  # return errors.numpy()\n","\n","  if error_fun == 'mse':\n","    errors = np.mean((data - decoded)**2, axis=(1,2))\n","  elif error_fun == 'mae':\n","    errors = np.mean(np.abs(data - decoded), axis=(1,2))\n","  \n","  return errors\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rG5GhWkR_AK7"},"source":["'''\n","compute the q-th quantile of the errors which serves as our\n","threshold to identify anomalies -- any data point that our model\n","reconstructed with > threshold error will be marked as an outlier\n","INPUT\n","data: scaled data with input shape that depends on the model\n","model: an autoencoder which will be used to reconstruct the input data\n","quantile: Above which quantile should the reconstruction error be in order to be considered an anomaly\n","error_fun: Mean Square Error or Mean Absolute Error\n","OUTPUT\n","anomaly_idxs: The indices of anomlous data\n","'''\n","def find_anomalies(data, model, quantile, error_fun='mse'):\n","  errors = find_errors(data, model, error_fun)\n","  thresh = np.quantile(errors, quantile)\n","  anomaly_idxs = np.where(np.array(errors) >= thresh)[0]\n","  return anomaly_idxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUuNLghA_WuC"},"source":["'''\n","(Not used in the final model)\n","This is for a specific recording (data) with specific quality_file. It doesn't take into consideration historical data\n","leftovers: How many data points do not have a corresponding quality label (obsolete)\n","'''\n","def performance(data, leftovers, model, quantile, quality_file, path, error_fun='mse', snippet_len=125):\n","\n","  errors = find_errors(data, model, error_fun)\n","  thresh = np.quantile(errors, quantile)\n","\n","  # idxs = predicted\n","  anomaly_idxs = np.where(np.array(errors) >= thresh)[0]\n","  normal_idxs = np.where(np.array(errors) < thresh)[0] \n","\n","  file_signal = h5py.File(quality_file, 'r')\n","  signal_quality = file_signal['data'][()]\n","  file_signal.close()\n","\n","  signal_quality = signal_quality.flatten()\n","  if leftovers != 0:\n","    signal_quality = signal_quality[data.shape[1]//125-1:-leftovers//125]\n","  else:\n","    signal_quality = signal_quality[data.shape[1]//125-1:]\n","\n","  # indexes = real\n","  anomaly_indices = np.where(signal_quality == 0)[0]\n","  normal_indices = np.where(signal_quality == 1)[0]\n","\n","  # positive = low quality = 0\n","  # negatice = normal quality = 1\n","\n","  tp = len(np.intersect1d(anomaly_indices, anomaly_idxs))\n","  fp = len(np.intersect1d(normal_indices, anomaly_idxs))\n","  tn = len(np.intersect1d(normal_indices, normal_idxs))\n","  fn = len(np.intersect1d(anomaly_indices, normal_idxs))\n","\n","  recall = tp/(tp+fn)\n","  precision = tp/(tp+fp)\n","  accuracy = (tp+tn)/(len(normal_indices) + len(anomaly_indices))\n","\n","  print('accuracy = %.2f \\nprecision = %.2f \\nrecall = %.2f' % (accuracy, precision, recall))\n","\n","  return accuracy, precision, recall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVBieYKU7KxH"},"source":["# (Not used in the final model)\n","# It runs the function performance over all recordings in a folder\n","def performance_per_test_file(model, quantile=0.90, path=r'Data/fetal_quality_assessment', error_fun='mse', snippet_len=125):\n","\n","  filenames = os.listdir(path + '/test')\n","\n","  # filename -> (tpr,tnr)\n","  perform = dict()\n","\n","  for name in tqdm(filenames):\n","    print(name)\n","    quality_file = name[:-12] + '.quality'\n","\n","    data, leftovers = create_dataset_from_file(path + '/test/' + name, snippet_len, sliding_window=True, trim_zeros=False, return_leftovers=True)\n","\n","    accuracy, precision, recall = performance(data, leftovers, model, quantile, path + '/quality_files/' + quality_file, path, error_fun, snippet_len)\n","\n","    perform[name] = accuracy, precision, recall\n","\n","  return perform\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_EpWtd_Fml4R"},"source":["'''\n","Present confusion matrix\n","INPUT\n","y_true: the true labels of the data (low quality -> 1, high quality -> 0)\n","y_likelihoods: The likelihood a sample is good or bad quality (a number between 0 and 1)\n","thr: The threshold above which the likelihood should be to consider a sample of bad quality (tunable)\n","''' \n","def confusion(y_true, y_likelihoods, thr=0.5):\n","\n","  \n","  good_indices_predicted = np.where(y_likelihoods < thr)[0]\n","  bad_indixes_predicted = np.where(y_likelihoods > thr)[0]\n","\n","  y_pred = np.zeros(y_true.size)\n","  y_pred[bad_indixes_predicted] = 1\n","\n","  LABELS = ['Good quality', 'Bad quality']\n","\n","  conf_matrix = confusion_matrix(y_true, y_pred)\n","  plt.figure(figsize=(16, 10))\n","  sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n","  plt.title(\"Confusion matrix\")\n","  plt.ylabel('True class')\n","  plt.xlabel('Predicted class')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skEToMU1gEWy"},"source":["'''\n","Present scatter plot where each sample is a point\n","INPUT\n","y_true: the true labels of the data (low quality -> 1, high quality -> 0)\n","y_likelihoods: The likelihood a sample is good or bad quality (a number between 0 and 1)\n","''' \n","def scatter(y_true, y_likelihoods):\n","\n","  good_indices = np.where(y_true == 0)[0]\n","  bad_indices = np.where(y_true == 1)[0]\n","  good_points = y_likelihoods[good_indices]\n","  bad_points = y_likelihoods[bad_indices]\n","\n","  plt.figure(figsize=(16,10))\n","  plt.scatter(good_indices, good_points, label='hign quality', c='b', marker='.')\n","  plt.scatter(bad_indices, bad_points, label='low quality', c='r', marker='.')\n","  plt.ylabel(\"Likelihood\")\n","  plt.xlabel(\"Data point index\")\n","\n","  plt.legend()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvN0T0lCrVFc"},"source":["'''\n","Present the precision-recall curve, in order to assess model and tune the threshold\n","INPUT\n","y_true: the true labels of the data (low quality -> 1, high quality -> 0)\n","y_likelihoods: The likelihood a sample is good or bad quality (a number between 0 and 1)\n","''' \n","def precision_recall_curv(y_true, y_likelihoods):\n","\n","  precision_rt, recall_rt, threshold_rt = precision_recall_curve(y_true, y_likelihoods)\n","  percentiles = np.argsort(threshold_rt) * 100. / (len(threshold_rt) - 1)\n","\n","  # find intersection\n","  idx = np.argwhere(np.diff(np.sign(precision_rt - recall_rt))).flatten()\n","  x = percentiles[idx[0]]\n","  y = precision_rt[idx[0]]\n","  print('(x,y)=(%.2f, %.2f)' % (x,y))\n","\n","  plt.figure(figsize=(16,10))\n","  plt.plot(percentiles, precision_rt[1:], label=\"Precision\", linewidth=2)\n","  plt.plot(percentiles, recall_rt[1:], label=\"Recall\", linewidth=2)\n","  plt.title('Precision and recall for different threshold values')\n","  plt.xlabel('Threshold')\n","  plt.ylabel('Precision/Recall')\n","  plt.legend()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWUKpONmsG39"},"source":["'''\n","Present the ROC curve and compute the AUC to assess the model\n","INPUT\n","y_true: the true labels of the data (low quality -> 1, high quality -> 0)\n","y_likelihoods: The likelihood a sample is good or bad quality (a number between 0 and 1)\n","''' \n","def roc_auc(y_true, y_likelihoods):\n","  false_pos_rate, true_pos_rate, thresholds = roc_curve(y_true, y_likelihoods)\n","  roc_auc = auc(false_pos_rate, true_pos_rate,)\n","\n","  plt.figure(figsize=(12, 12))\n","  plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n","  plt.plot([0,1],[0,1], linewidth=5)\n","  plt.xlim([-0.01, 1])\n","  plt.ylim([0, 1.01])\n","  plt.legend(loc='lower right')\n","  plt.title('Receiver operating characteristic curve (ROC)')\n","  plt.ylabel('True Positive Rate')\n","  plt.xlabel('False Positive Rate')\n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGH0rcdxaqCp"},"source":["'''\n","Calculate precision, recall and accuracy\n","INPUT\n","y_true: the true labels of the data (low quality -> 1, high quality -> 0)\n","y_likelihoods: The likelihood a sample is good or bad quality (a number between 0 and 1)\n","thr: The threshold above which the likelihood should be to consider a sample of bad quality (tunable)\n","''' \n","def precision_recall_metrics(y_true, y_likelihoods, thr):\n","\n","   \n","  good_indices_predicted = np.where(y_likelihoods < thr)[0]\n","  bad_indixes_predicted = np.where(y_likelihoods > thr)[0]\n","\n","  y_pred = np.zeros(y_true.size)\n","  y_pred[bad_indixes_predicted] = 1\n","\n","  total = y_true.size\n","  real_pos = np.where(y_true == 1)[0]\n","  real_neg = np.where(y_true == 0)[0]\n","  pred_pos = np.where(y_pred == 1)[0]\n","  pred_neg = np.where(y_pred == 0)[0]\n","\n","  tp = np.intersect1d(real_pos, pred_pos).size\n","  tn = np.intersect1d(real_neg, pred_neg).size\n","  fp = np.intersect1d(real_neg, pred_pos).size\n","  fn = np.intersect1d(real_pos, pred_neg).size\n","\n","  precision = tp/(tp+fp)\n","  recall = tp/(tp+fn)\n","  accuracy = (tp+tn)/total\n","\n","  print('Precision = %.4f \\nRecall = %.4f \\nAccuracy = %.4f' % (precision, recall, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_WE_8FCQi4N"},"source":["Previous versions of performance metrics functions"]},{"cell_type":"code","metadata":{"id":"joiEszdPOwSm"},"source":["# def scatter_plot(model, file_path, quality_path, quantile=None, snippet_len=125, scaler_path='Models/ALL_scaler.pkl'):\n","\n","#   scaler = load(open(scaler_path, 'rb'))\n","#   d, leftovers = create_dataset_from_file(file_path, snippet_len, sliding_window=True, trim_zeros=False, return_leftovers=True)\n","#   d = scale_data(d, scaler, 125)\n","#   errors = find_errors(d, model)\n","#   segment_errors=errors[:, -125:]\n","#   mean_errors = np.mean(segment_errors, axis=1)\n","\n","#   file_signal = h5py.File(quality_path, 'r')\n","#   signal_quality = file_signal['data'][()]\n","#   file_signal.close()\n","\n","#   signal_quality = remove_nan(signal_quality)\n","#   signal_quality = signal_quality.flatten()\n","\n","#   if leftovers!=0:\n","#     signal_quality = signal_quality[snippet_len//125-1:-leftovers//125]\n","#   else:\n","#     signal_quality = signal_quality[snippet_len//125-1:]\n","\n","#   bad_indices = np.where(signal_quality == 0)[0]\n","#   good_indices = np.where(signal_quality == 1)[0]\n","\n","#   good_points = mean_errors[good_indices]\n","#   bad_points = mean_errors[bad_indices]\n","\n","#   plt.figure(figsize=(16,10))\n","#   # plt.xlim(6000, 8000)\n","#   # plt.ylim(0, 100)\n","#   plt.scatter(good_indices, good_points, label='hign quality', c='b')\n","#   plt.scatter(bad_indices, bad_points, label='low quality', c='r')\n","#   plt.title(\"Reconstruction error\")\n","#   plt.ylabel(\"Reconstruction error\")\n","#   plt.xlabel(\"Data point index\")\n","\n","#   if quantile is not None:\n","#     thr = np.quantile(mean_errors, quantile)\n","#     plt.hlines(thr, 0, len(signal_quality), colors=\"g\", zorder=100, label='Threshold')\n","\n","#   plt.legend()\n","#   plt.show()\n","\n","\n","\n","\n","\n","# def precision_recall(model, file_path, quality_path, snippet_len=125, scaler_path='Models/ALL_scaler.pkl'):\n","\n","#   scaler = load(open(scaler_path, 'rb'))\n","#   d, leftovers = create_dataset_from_file(file_path, snippet_len, sliding_window=True, trim_zeros=False, return_leftovers=True)\n","#   d = scale_data(d, scaler, 125)\n","#   errors = find_errors(d, model)\n","#   segment_errors=errors[:, -125:]\n","#   mean_errors = np.mean(segment_errors, axis=1)\n","\n","#   file_signal = h5py.File(quality_path, 'r')\n","#   signal_quality = file_signal['data'][()]\n","#   file_signal.close()\n","\n","#   signal_quality = remove_nan(signal_quality)\n","#   signal_quality = signal_quality.flatten()\n","\n","#   if leftovers!=0:\n","#     signal_quality = signal_quality[snippet_len//125-1:-leftovers//125]\n","#   else:\n","#     signal_quality = signal_quality[snippet_len//125-1:]\n","\n","#   y_true = np.logical_xor(signal_quality, np.ones(len(signal_quality)))  # because we consider the bad ones positives\n","#   precision_rt, recall_rt, threshold_rt = precision_recall_curve(y_true, mean_errors)\n","#   percentiles = np.argsort(threshold_rt) * 100. / (len(threshold_rt) - 1)\n","#   plt.figure(figsize=(16,10))\n","#   plt.plot(percentiles, precision_rt[1:], label=\"Precision\",linewidth=2)\n","#   plt.plot(percentiles, recall_rt[1:], label=\"Recall\",linewidth=2)\n","#   plt.title('Precision and recall for different threshold values')\n","#   plt.xlabel('Threshold')\n","#   plt.ylabel('Precision/Recall')\n","#   plt.legend()\n","#   plt.show()\n","\n","\n","\n","\n","# def confusion(model, file_path, quality_path, quantile, snippet_len=125, scaler_path='Models/ALL_scaler.pkl'):\n","\n","#   scaler = load(open(scaler_path, 'rb'))\n","#   d, leftovers = create_dataset_from_file(file_path, snippet_len, sliding_window=True, trim_zeros=False, return_leftovers=True)\n","#   d = scale_data(d, scaler, 125)\n","#   errors = find_errors(d, model)\n","#   segment_errors=errors[:, -125:]\n","#   mean_errors = np.mean(segment_errors, axis=1)\n","\n","#   file_signal = h5py.File(quality_path, 'r')\n","#   signal_quality = file_signal['data'][()]\n","#   file_signal.close()\n","\n","#   signal_quality = remove_nan(signal_quality)\n","#   signal_quality = signal_quality.flatten()\n","\n","#   if leftovers!=0:\n","#     signal_quality = signal_quality[snippet_len//125-1:-leftovers//125]\n","#   else:\n","#     signal_quality = signal_quality[snippet_len//125-1:]\n","\n","#   thresh = np.quantile(mean_errors, quantile)\n","#   anomaly_idxs = np.where(np.array(mean_errors) >= thresh)[0]\n","#   y_pred = np.zeros(len(signal_quality))\n","#   y_pred[anomaly_idxs] = 1\n","#   y_true = np.logical_xor(signal_quality, np.ones(len(signal_quality)))  # because we consider the bad ones positives\n","#   LABELS = ['Good quality', 'Bad quality']\n","\n","#   conf_matrix = confusion_matrix(y_true, y_pred)\n","#   plt.figure(figsize=(16, 10))\n","#   sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n","#   plt.title(\"Confusion matrix\")\n","#   plt.ylabel('True class')\n","#   plt.xlabel('Predicted class')\n","#   plt.show()\n","\n","\n","\n","# def roc_auc(model, file_path, quality_path, snippet_len=125, scaler_path='Models/ALL_scaler.pkl'):\n","\n","#   scaler = load(open(scaler_path, 'rb'))\n","#   d, leftovers = create_dataset_from_file(file_path, snippet_len, sliding_window=True, trim_zeros=False, return_leftovers=True)\n","#   d = scale_data(d, scaler, 125)\n","#   errors = find_errors(d, model)\n","#   segment_errors=errors[:, -125:]\n","#   mean_errors = np.mean(segment_errors, axis=1)\n","\n","#   file_signal = h5py.File(quality_path, 'r')\n","#   signal_quality = file_signal['data'][()]\n","#   file_signal.close()\n","\n","#   signal_quality = remove_nan(signal_quality)\n","#   signal_quality = signal_quality.flatten()\n","\n","#   if leftovers!=0:\n","#     signal_quality = signal_quality[snippet_len//125-1:-leftovers//125]\n","#   else:\n","#     signal_quality = signal_quality[snippet_len//125-1:]\n","\n","#   y_true = np.logical_xor(signal_quality, np.ones(len(signal_quality)))  # because we consider the bad ones positives\n","\n","#   false_pos_rate, true_pos_rate, thresholds = roc_curve(y_true, mean_errors)\n","#   roc_auc = auc(false_pos_rate, true_pos_rate,)\n","\n","#   plt.figure(figsize=(12, 12))\n","#   plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n","#   plt.plot([0,1],[0,1], linewidth=5)\n","#   plt.xlim([-0.01, 1])\n","#   plt.ylim([0, 1.01])\n","#   plt.legend(loc='lower right')\n","#   plt.title('Receiver operating characteristic curve (ROC)')\n","#   plt.ylabel('True Positive Rate')\n","#   plt.xlabel('False Positive Rate')\n","#   plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]}]}